# -*- coding: utf-8 -*-
"""forest_U_Net_pytorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EftsmqL1ZmoaGnU2Oh_Wh7vrivd4rjpl


①データの確認、探索  
②データの前処理  
③U-Netのモデルの定義、トレーニング  
④U-Netモデルの性能評価の確認
"""
import os, glob
import re, shutil
from tqdm import tqdm
import time
import copy
from collections import defaultdict
import torch
import shutil
import pandas as pd
from skimage import io, transform
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader, random_split
from torchvision import transforms, utils
from torch import nn
import albumentations as A
from albumentations.pytorch.transforms import ToTensorV2 as ToTensor
from tqdm import tqdm as tqdm

from albumentations import (HorizontalFlip, ShiftScaleRotate, Normalize, Resize, Compose, GaussNoise)
import cv2

from torch.autograd import Variable
from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout
from torch.optim import Adam, SGD
import torch.nn.functional as F
from PIL import Image
from torch import nn
import zipfile
from fpathutils import get_mskPath
import random

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

# ②データの前処理  
#画像データ拡張の関数
def get_train_transform(resizeValue):
   return A.Compose(
       [
        #リサイズ(こちらはすでに適用済みなのでなくても良いです)
        A.Resize(resizeValue, resizeValue),
        #正規化(こちらの細かい値はalbumentations.augmentations.transforms.Normalizeのデフォルトの値を適用)
        # A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
        A.Normalize(),
        #水平フリップ（pはフリップする確率）
        A.HorizontalFlip(p=0.25),
        #垂直フリップ
        A.VerticalFlip(p=0.25),
        ToTensor()
        ])

def mask2single(mask,values:list):
    for value in values:
        mask[mask==value] = 255
    mask[mask!=255] = 0
    return mask



#Datasetクラスの定義
class LoadDataSet(Dataset):
        def __init__(self,path,resizeValue, transform=None):
            self.path = path
            self.folders = os.listdir(path)
            self.transforms = get_train_transform(resizeValue)
            self.resizeValue = resizeValue
        
        def __len__(self):
            return len(self.folders)
              
        
        def __getitem__(self,idx):
            image_path = os.path.join(self.path, self.folders[idx])
            mask_path = get_mskPath(os.path.join(self.path, self.folders[idx]))
            resizeValue = self.resizeValue
            # image_path = os.path.join(image_folder,os.listdir(image_folder)[idx])
            
            #画像データの取得
            img = io.imread(image_path)[:,:,0:3].astype('float32')
            img = transform.resize(img,(resizeValue,resizeValue))
            
            mask = self.get_mask(mask_path, resizeValue, resizeValue ).astype('float32')


            augmented = self.transforms(image=img, mask=mask)
            img = augmented['image']
            mask = augmented['mask']
            mask = mask.permute(2, 0, 1)


            # # 可視化
            # figure, ax = plt.subplots(nrows=2, ncols=2, figsize=(5, 8))
            # ax[0,0].imshow(img.permute(1, 2, 0))#img画像は正規化しているため色味がおかしい
            # ax[0,1].imshow(mask[0,:,:])

            return (img,mask) 

        #マスクデータの取得
        def get_mask(self,mask_path,IMG_HEIGHT, IMG_WIDTH):
            mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool_)
            # for mask_ in os.listdir(mask_folder):
            mask_ = io.imread(mask_path)
            mask_ = transform.resize(mask_, (IMG_HEIGHT, IMG_WIDTH))
            mask_ = np.expand_dims(mask_,axis=-1)
            mask = np.maximum(mask, mask_)              
            return mask


# train_dataset = LoadDataSet(TRAIN_PATH, transform=get_train_transform())

# # 一枚の画像データとマスクの次元を確認
# image, mask = train_dataset.__getitem__(3)
# print(image.shape)
# print(mask.shape)

# # 画像枚数を確認
# train_dataset.__len__()


"""次に、入力画像とマスクのデータがどうなっているのか確認してみます。"""

def format_image(img):
    img = np.array(np.transpose(img, (1,2,0)))
    #下は画像拡張での正規化を元に戻しています
    mean=np.array((0.485, 0.456, 0.406))
    std=np.array((0.229, 0.224, 0.225))
    img  = std * img + mean
    img = img*255
    img = img.astype(np.uint8)
    return img


def format_mask(mask):
    mask = np.squeeze(np.transpose(mask, (1,2,0)))
    return mask

plt.clf()

def visualize_dataset(n_images, predict=None):
    images = random.sample(range(0, train_dataset.__len__()), n_images)
    figure, ax = plt.subplots(nrows=len(images), ncols=2, figsize=(5, 8))
    # print(images)
    for i in range(0, len(images)):
        img_no = images[i]
        # print(img_no)
        image, mask = train_dataset.__getitem__(img_no)
        image = format_image(image)
        mask = format_mask(mask)
        ax[i, 0].imshow(image)
        ax[i, 1].imshow(mask, interpolation="nearest", cmap="gray")
        ax[i, 0].set_title("Input Image")
        ax[i, 1].set_title("Label Mask")
        ax[i, 0].set_axis_off()
        ax[i, 1].set_axis_off()
    plt.tight_layout()
    plt.show()


# 続いてU-Netのモデルを実装します。モデルについては解説記事か以下のサイトをご参照ください。
# https://www.researchgate.net/figure/U-net-Convolutional-Neural-Network-model-The-U-net-model-contains-two-parts_fig6_317493482
# 　　　 
# 
# こちらを元に実装をしていきます。

# U-Netモデルにおいては細かい構成というよりはモデルの全体構成から把握していった方が理解がしやすいと思いました。
# 
# U-Net解説記事にも記載してあります通り、
# 
# ①FCNにあたる部分、
# 
# ②Up Samplingにあたる部分、
# 
# ③Skip Connectionにあたる部分
# 
# をまず把握します。
# 
# 以下のコードコメント文にそれぞれがどこに該当するかを記載しています。
# 
# Skip Connectionはtorch.catによりFCN時の出力と合わせています。
# 
# conv_bn_relu関数は畳み込みとバッチ正規化と活性化関数Reluをまとめています。

# セマンティックセグメンテーションの損失関数としてはBCELoss(Binary Cross Entropy)をベースとしたDiceBCELossがよく用いられます。詳細な説明とコードは以下に記載があります。https://www.kaggle.com/bigironsphere/loss-function-library-keras-pytorch
# 
# 考え方としてはIoUに近く、予測した範囲が過不足なく教師データとなる領域を捉えているほど損失が低くなります。

# UNet 
class UNet(nn.Module):
    def __init__(self, input_channels, output_channels):
        super().__init__()
        # 資料中の『FCN』に当たる部分
        self.conv1 = conv_bn_relu(input_channels,64)
        self.conv2 = conv_bn_relu(64, 128)
        self.conv3 = conv_bn_relu(128, 256)
        self.conv4 = conv_bn_relu(256, 512)
        self.conv5 = conv_bn_relu(512, 1024)
        self.down_pooling = nn.MaxPool2d(2)

        # 資料中の『Up Sampling』に当たる部分
        self.up_pool6 = up_pooling(1024, 512)
        self.conv6 = conv_bn_relu(1024, 512)
        self.up_pool7 = up_pooling(512, 256)
        self.conv7 = conv_bn_relu(512, 256)
        self.up_pool8 = up_pooling(256, 128)
        self.conv8 = conv_bn_relu(256, 128)
        self.up_pool9 = up_pooling(128, 64)
        self.conv9 = conv_bn_relu(128, 64)
        self.conv10 = nn.Conv2d(64, output_channels, 1)

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_out')
                if m.bias is not None:
                    m.bias.data.zero_()

    def forward(self, x):
        # 正規化
        x = x/255.

        # 資料中の『FCN』に当たる部分
        x1 = self.conv1(x)
        p1 = self.down_pooling(x1)
        x2 = self.conv2(p1)
        p2 = self.down_pooling(x2)
        x3 = self.conv3(p2)
        p3 = self.down_pooling(x3)
        x4 = self.conv4(p3)
        p4 = self.down_pooling(x4)
        x5 = self.conv5(p4)

        # 資料中の『Up Sampling』に当たる部分, torch.catによりSkip Connectionをしている
        p6 = self.up_pool6(x5)
        x6 = torch.cat([p6, x4], dim=1)
        x6 = self.conv6(x6)

        p7 = self.up_pool7(x6)
        x7 = torch.cat([p7, x3], dim=1)
        x7 = self.conv7(x7)

        p8 = self.up_pool8(x7)
        x8 = torch.cat([p8, x2], dim=1)
        x8 = self.conv8(x8)

        p9 = self.up_pool9(x8)
        x9 = torch.cat([p9, x1], dim=1)
        x9 = self.conv9(x9)

        output = self.conv10(x9)
        output = torch.sigmoid(output)

        return output

#畳み込みとバッチ正規化と活性化関数Reluをまとめている
def conv_bn_relu(in_channels, out_channels, kernel_size=3, stride=1, padding=1):
    return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
    )

def down_pooling():
    return nn.MaxPool2d(2)

def up_pooling(in_channels, out_channels, kernel_size=2, stride=2):
    return nn.Sequential(
        #転置畳み込み
        nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride),
        nn.BatchNorm2d(out_channels),
        nn.ReLU(inplace=True)
    )

"""## 損失関数について

セマンティックセグメンテーションの損失関数としてはBCELoss(Binary Cross Entropy)をベースとしたDiceBCELossがよく用いられます。詳細な説明とコードは以下に記載があります。https://www.kaggle.com/bigironsphere/loss-function-library-keras-pytorch
考え方としてはIoUに近く、予測した範囲が過不足なく教師データとなる領域を捉えているほど損失が低くなります。
"""

class DiceBCELoss(nn.Module):
    def __init__(self, weight=None, size_average=True):
        super(DiceBCELoss, self).__init__()

    def forward(self, inputs, targets, smooth=1):
        
        #comment out if your model contains a sigmoid or equivalent activation layer
        inputs = F.sigmoid(inputs)       
        
        #flatten label and prediction tensors
        inputs = inputs.view(-1)
        targets = targets.view(-1)
        
        intersection = (inputs * targets).sum()                            
        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  
        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')
        Dice_BCE = BCE + dice_loss
        
        return Dice_BCE


class DiceLoss(nn.Module):
    def __init__(self, weight=None, size_average=True):
        super(DiceLoss, self).__init__()

    def forward(self, inputs, targets, smooth=1):
        
        #comment out if your model contains a sigmoid or equivalent activation layer
        #inputs = F.sigmoid(inputs)       
        
        #flatten label and prediction tensors
        inputs = inputs.view(-1)
        targets = targets.view(-1)
        
        intersection = (inputs * targets).sum()                            
        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  
        
        return 1 - dice

"""セマンティックセグメンテーションの精度評価指標となるIoUのクラスを定義します。"""

class IoU(nn.Module):
    def __init__(self, weight=None, size_average=True):
        super(IoU, self).__init__()

    def forward(self, inputs, targets, smooth=1):
        inputs = inputs.view(-1)
        targets = targets.view(-1)
        intersection = (inputs * targets).sum()
        total = (inputs + targets).sum()
        union = total - intersection 
        
        IoU = (intersection + smooth)/(union + smooth)
                
        return IoU

"""続いてモデルを保存、ロードするための関数を定義します。"""

def save_ckp(state, is_best, checkpoint_path, best_model_path):
    f_path = checkpoint_path
    torch.save(state, f_path)
    if is_best:
        best_fpath = best_model_path
        shutil.copyfile(f_path, best_fpath)

def load_ckp(checkpoint_fpath, model, optimizer):
    checkpoint = torch.load(checkpoint_fpath)
    model.load_state_dict(checkpoint['state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer'])
    valid_loss_min = checkpoint['valid_loss_min']
    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()


def getImgNo_fromImgPaths(paths):
    # Regular expression pattern
    pattern = re.compile(r'trainOutImages\\(\d+)_3_pred.png')

    # List to store the numbers
    numbers = []

    for path in paths:
        match = pattern.search(path)
        if match:
            number = int(match.group(1))  # convert the extracted string to int
            numbers.append(number)

    return numbers



# !pip install seaborn
import seaborn as sns

def plotScore(csvPath:str, strSize:int):

    df = pd.read_csv(csvPath)
    plt.figure(1)
    plt.figure(figsize=(15,5))
    sns.set_style(style="darkgrid")
    plt.subplot(1, 2, 1)
    sns.lineplot(x=range(1,len(df)+1), y=df["train_Loss"], label="Train Loss")
    sns.lineplot(x=range(1,len(df)+1), y=df["valid__Loss"], label="Valid Loss")
    # plt.title("Loss")
    plt.legend(fontsize=strSize)

    lossYrangeMax = df[["train_Loss",	"valid__Loss"]].max().max()
    lossYrangeMin = df[["train_Loss",	"valid__Loss"]].min().min()
    lossYrangeMax = round(lossYrangeMax, 2)
    lossYrangeMin = round(lossYrangeMin, 2)
    print(lossYrangeMin, lossYrangeMax)
    plt.yticks(np.arange(lossYrangeMin, lossYrangeMax+0.1, step=0.2))
    plt.title("Loss")
    plt.xlabel("epochs",fontsize=strSize)
    plt.ylabel("DiceLoss",fontsize=strSize,labelpad=-20)
    plt.tick_params(labelsize=strSize)


    plt.subplot(1, 2, 2)
    sns.lineplot(x=range(1,len(df)+1), y=df["train_scoreIoU"], label="Train Score")
    sns.lineplot(x=range(1,len(df)+1), y=df["valid__scoreIoU"], label="Valid Score")
    # plt.title("Score (IoU)")
    plt.yticks(np.arange(0, 1.1, step=0.1))
    plt.legend(fontsize=strSize)
    plt.title("Score (IoU)")
    plt.xlabel("epochs",fontsize=strSize)
    plt.ylabel("IoU",fontsize=strSize,labelpad=-30)
    plt.tick_params(labelsize=strSize)

    figPath = os.path.join(os.path.dirname(csvPath),f"Unet_score_{os.path.basename(csvPath)}.png")
    plt.savefig(figPath,facecolor="azure", bbox_inches='tight', pad_inches=0)
    print(figPath)
    # plt.show()



def visualize_training_predict(org_batchs, msk_batchs, pred_batchs, workDir:str, maskRGB:bool,imgSave:bool):

    figure, ax = plt.subplots(nrows=1, ncols=3, figsize=(6, 2))

    batchsize = org_batchs.shape[0]
    random_index = np.random.randint(batchsize)

    # for img_no in tqdm(range(0, n_images)):
    tm  = pred_batchs[random_index].data.cpu().numpy()
    tm  = tm.squeeze()
    img = org_batchs[random_index].data.cpu()
    msk = msk_batchs[random_index].data.cpu()
    img = format_image(img)
    msk = format_mask(msk)

    # print(tm.shape, img.shape, msk.shape)
    if maskRGB:
        tm = img * tm[:, :, np.newaxis]
        msk = img * msk[:, :, np.newaxis]
    else:
        msk[msk==1]=255
        tm[tm==1]=255

    # msk = cv2.medianBlur(np.array(msk),11)

    img_no = 0    

    outImagePath_img = os.path.join(workDir,"trainOutImages",f"{str(img_no)}_1_org.png")
    outImagePath_msk = os.path.join(workDir,"trainOutImages",f"{str(img_no)}_2_msk.png")
    outImagePath_pred = os.path.join(workDir,"trainOutImages",f"{str(img_no)}_3_pred.png")
    os.makedirs(os.path.dirname(outImagePath_pred),exist_ok=True)

    if os.path.exists(outImagePath_pred):
        imgNoList = getImgNo_fromImgPaths(glob.glob(os.path.join(workDir,"trainOutImages", "*pred.png")))
        if len(imgNoList)!=0:
            img_no = sorted(imgNoList)[-1] + 1
            outImagePath_img = os.path.join(workDir,"trainOutImages",f"{str(img_no)}_1_org.png")
            outImagePath_msk = os.path.join(workDir,"trainOutImages",f"{str(img_no)}_2_msk.png")
            outImagePath_pred = os.path.join(workDir,"trainOutImages",f"{str(img_no)}_3_pred.png")


    # print(tm.shape, img.shape, msk.shape)
    cv2.imwrite(outImagePath_img, img)
    cv2.imwrite(outImagePath_msk,np.array(msk))
    cv2.imwrite(outImagePath_pred,tm)


    # ax[0].imshow(img)
    # ax[1].imshow(msk)
    # ax[2].imshow(tm)
    # # ax[1].imshow(msk, interpolation="nearest", cmap="gray")
    # # ax[2].imshow(tm, interpolation="nearest", cmap="gray")
    # ax[0].set_title("Input")
    # ax[1].set_title("Mask")
    # ax[2].set_title("Predict")
    # ax[0].set_axis_off()
    # ax[1].set_axis_off()
    # ax[2].set_axis_off()
    # plt.tight_layout()
    # if imgSave:
    #     plt.savefig(os.path.join(workDir,"trainOutImages",f"predictedSet_{img_no}.png"))
    # # plt.show()
    # del ax, figure, tm, img, msk
    # plt.clf()
    # plt.close()