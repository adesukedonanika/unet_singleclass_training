# -*- coding: utf-8 -*-
"""forest_U_Net_pytorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EftsmqL1ZmoaGnU2Oh_Wh7vrivd4rjpl


①データの確認、探索  
②データの前処理  
③U-Netのモデルの定義、トレーニング  
④U-Netモデルの性能評価の確認
"""
import os, glob
import re, shutil
from tqdm import tqdm
import time
import copy
from collections import defaultdict
import torch
import shutil
import pandas as pd
from skimage import io, transform
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader, random_split
from torchvision import transforms, utils
from torch import nn
import albumentations as A
from albumentations.pytorch.transforms import ToTensorV2 as ToTensor
from tqdm import tqdm as tqdm

from albumentations import (HorizontalFlip, ShiftScaleRotate, Normalize, Resize, Compose, GaussNoise)
import cv2

from torch.autograd import Variable
from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout
from torch.optim import Adam, SGD
import torch.nn.functional as F
from PIL import Image
from torch import nn
import zipfile

import random

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
print(torch.cuda.is_available(),device)


## ①データの確認、探索
workDir = "./training"
TRAIN_PATH = workDir + '/hikariTestAreaKGL/'
os.makedirs(TRAIN_PATH, exist_ok=True)


classValuesDict = {"Conifer":[1,11]}
# classValuesDict = {"Bamboo":[33]}
classValues = [float(value) for value in classValuesDict["Conifer"]]
classValues

className = str(list(classValuesDict.keys())[0])

TRAIN_PATH = workDir + '/hikariTestAreaKGL/'
TRAIN_PATH = os.path.join(TRAIN_PATH,"256rap192")
os.makedirs(TRAIN_PATH,exist_ok=True)

remakeFolder = "./training/hikariTestArea/org/256rap192__2445_50cm_hikariTestArea"

for fileName in tqdm(glob.glob(remakeFolder + "/*.tif")):

# print(fileNames)
    imageID = re.findall(r'.*hikariTestArea_(\d+_\d+)\.tif', fileName)[0]

    imageID = imageID + f"_{className}"

    orgIamgeDirPath = os.path.join(TRAIN_PATH,imageID,"org")
    os.makedirs(orgIamgeDirPath,exist_ok=True) 
    mskImageDirPath = orgIamgeDirPath.replace("org","msk")
    os.makedirs(mskImageDirPath,exist_ok=True) 

    orgPath = os.path.join(orgIamgeDirPath,os.path.basename(fileName))
    mskPath = os.path.join(mskImageDirPath,os.path.basename(fileName.replace("org","msk")))

    if not os.path.exists(orgPath):
        shutil.copy(fileName, orgPath)
    if not os.path.exists(mskPath):
        shutil.copy(fileName.replace("org","msk"), mskPath)


# ②データの前処理  
#画像データ拡張の関数
def get_train_transform():
   return A.Compose(
       [
        #リサイズ(こちらはすでに適用済みなのでなくても良いです)
        A.Resize(256, 256),
        #正規化(こちらの細かい値はalbumentations.augmentations.transforms.Normalizeのデフォルトの値を適用)
        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
        #水平フリップ（pはフリップする確率）
        A.HorizontalFlip(p=0.25),
        #垂直フリップ
        A.VerticalFlip(p=0.25),
        ToTensor()
        ])

def mask2single(mask,values:list):
    for value in values:
        mask[mask==value] = 255
    mask[mask!=255] = 0
    return mask


#Datasetクラスの定義
class LoadDataSet(Dataset):
        def __init__(self,path, transform=None):
            self.path = path
            self.folders = os.listdir(path)
            self.transforms = get_train_transform()
        
        def __len__(self):
            return len(self.folders)
              
        
        def __getitem__(self,idx):
            image_folder = os.path.join(self.path, self.folders[idx], "org")
            mask_folder = os.path.join(self.path, self.folders[idx], "msk")
            image_path = os.path.join(image_folder,os.listdir(image_folder)[0])
            
            #画像データの取得
            img = io.imread(image_path)[:,:,0:3].astype('float32')
            img = transform.resize(img,(256,256))
            
            mask = self.get_mask(mask_folder, 256, 256 ).astype('float32')


            augmented = self.transforms(image=img, mask=mask)
            img = augmented['image']
            mask = augmented['mask']
            mask = mask.permute(2, 0, 1)

            mask = mask2single(mask, classValues)

            # # 可視化
            # figure, ax = plt.subplots(nrows=2, ncols=2, figsize=(5, 8))
            # ax[0,0].imshow(img.permute(1, 2, 0))#img画像は正規化しているため色味がおかしい
            # ax[0,1].imshow(mask[0,:,:])

            return (img,mask) 

        #マスクデータの取得
        def get_mask(self,mask_folder,IMG_HEIGHT, IMG_WIDTH):
            mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)
            for mask_ in os.listdir(mask_folder):
                    mask_ = io.imread(os.path.join(mask_folder,mask_))
                    mask_ = transform.resize(mask_, (IMG_HEIGHT, IMG_WIDTH))
                    mask_ = np.expand_dims(mask_,axis=-1)
                    mask = np.maximum(mask, mask_)
              
            return mask

train_dataset = LoadDataSet(TRAIN_PATH, transform=get_train_transform())

# 一枚の画像データとマスクの次元を確認
image, mask = train_dataset.__getitem__(3)
print(image.shape)
print(mask.shape)

# 画像枚数を確認
train_dataset.__len__()


"""次に、入力画像とマスクのデータがどうなっているのか確認してみます。"""
def format_image(img):
    img = np.array(np.transpose(img, (1,2,0)))
    #下は画像拡張での正規化を元に戻しています
    mean=np.array((0.485, 0.456, 0.406))
    std=np.array((0.229, 0.224, 0.225))
    img  = std * img + mean
    img = img*255
    img = img.astype(np.uint8)
    return img
def format_mask(mask):
    mask = np.squeeze(np.transpose(mask, (1,2,0)))
    return mask


plt.clf()

def visualize_dataset(n_images, predict=None):
    images = random.sample(range(0, train_dataset.__len__()), n_images)
    figure, ax = plt.subplots(nrows=len(images), ncols=2, figsize=(5, 8))
    # print(images)
    for i in range(0, len(images)):
        img_no = images[i]
        # print(img_no)
        image, mask = train_dataset.__getitem__(img_no)
        image = format_image(image)
        mask = format_mask(mask)
        ax[i, 0].imshow(image)
        ax[i, 1].imshow(mask, interpolation="nearest", cmap="gray")
        ax[i, 0].set_title("Input Image")
        ax[i, 1].set_title("Label Mask")
        ax[i, 0].set_axis_off()
        ax[i, 1].set_axis_off()
    plt.tight_layout()
    plt.show()

# visualize_dataset(4)


## ②データの前処理
from genericpath import exists

# 学習と評価に使用するデータ量の割合　
# 0.2の場合、学習：80％、評価20％　
split_ratio = 0.2
train_size=int(np.round(train_dataset.__len__()*(1 - split_ratio),0))
valid_size=int(np.round(train_dataset.__len__()*split_ratio,0))

# バッチサイズは自動調整
BATCHSIZE = train_dataset.__len__()//20
# BATCHSIZE = 8
train_data, valid_data = random_split(train_dataset, [train_size, valid_size])
train_loader = DataLoader(dataset=train_data, batch_size=BATCHSIZE, shuffle=True)
val_loader = DataLoader(dataset=valid_data, batch_size=BATCHSIZE)

# エポック回数
num_epochs=100
modelID = "class{}_data{}_batch{}_epoch{}".format(className,train_dataset.__len__(), BATCHSIZE, num_epochs)

workDir = os.path.join(workDir, modelID)
os.makedirs(workDir, exist_ok=True)

print("Length of train　data:\t\t{}".format(len(train_data)))
print("Length of validation　data:\t{}".format(len(valid_data)))
print("Length of ALL　data:\t\t{}".format(train_dataset.__len__()))

"""## ③U-Netのモデルの定義、トレーニング

続いてU-Netのモデルを実装します。モデルについては解説記事か以下のサイトをご参照ください。
https://www.researchgate.net/figure/U-net-Convolutional-Neural-Network-model-The-U-net-model-contains-two-parts_fig6_317493482
　　　 

こちらを元に実装をしていきます。

U-Netモデルにおいては細かい構成というよりはモデルの全体構成から把握していった方が理解がしやすいと思いました。

U-Net解説記事にも記載してあります通り、

①FCNにあたる部分、

②Up Samplingにあたる部分、

③Skip Connectionにあたる部分

をまず把握します。

以下のコードコメント文にそれぞれがどこに該当するかを記載しています。

Skip Connectionはtorch.catによりFCN時の出力と合わせています。

conv_bn_relu関数は畳み込みとバッチ正規化と活性化関数Reluをまとめています。
"""

# UNet 
class UNet(nn.Module):
    def __init__(self, input_channels, output_channels):
        super().__init__()
        # 資料中の『FCN』に当たる部分
        self.conv1 = conv_bn_relu(input_channels,64)
        self.conv2 = conv_bn_relu(64, 128)
        self.conv3 = conv_bn_relu(128, 256)
        self.conv4 = conv_bn_relu(256, 512)
        self.conv5 = conv_bn_relu(512, 1024)
        self.down_pooling = nn.MaxPool2d(2)

        # 資料中の『Up Sampling』に当たる部分
        self.up_pool6 = up_pooling(1024, 512)
        self.conv6 = conv_bn_relu(1024, 512)
        self.up_pool7 = up_pooling(512, 256)
        self.conv7 = conv_bn_relu(512, 256)
        self.up_pool8 = up_pooling(256, 128)
        self.conv8 = conv_bn_relu(256, 128)
        self.up_pool9 = up_pooling(128, 64)
        self.conv9 = conv_bn_relu(128, 64)
        self.conv10 = nn.Conv2d(64, output_channels, 1)

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_out')
                if m.bias is not None:
                    m.bias.data.zero_()

    def forward(self, x):
        # 正規化
        x = x/255.

        # 資料中の『FCN』に当たる部分
        x1 = self.conv1(x)
        p1 = self.down_pooling(x1)
        x2 = self.conv2(p1)
        p2 = self.down_pooling(x2)
        x3 = self.conv3(p2)
        p3 = self.down_pooling(x3)
        x4 = self.conv4(p3)
        p4 = self.down_pooling(x4)
        x5 = self.conv5(p4)

        # 資料中の『Up Sampling』に当たる部分, torch.catによりSkip Connectionをしている
        p6 = self.up_pool6(x5)
        x6 = torch.cat([p6, x4], dim=1)
        x6 = self.conv6(x6)

        p7 = self.up_pool7(x6)
        x7 = torch.cat([p7, x3], dim=1)
        x7 = self.conv7(x7)

        p8 = self.up_pool8(x7)
        x8 = torch.cat([p8, x2], dim=1)
        x8 = self.conv8(x8)

        p9 = self.up_pool9(x8)
        x9 = torch.cat([p9, x1], dim=1)
        x9 = self.conv9(x9)

        output = self.conv10(x9)
        output = torch.sigmoid(output)

        return output

#畳み込みとバッチ正規化と活性化関数Reluをまとめている
def conv_bn_relu(in_channels, out_channels, kernel_size=3, stride=1, padding=1):
    return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
    )

def down_pooling():
    return nn.MaxPool2d(2)

def up_pooling(in_channels, out_channels, kernel_size=2, stride=2):
    return nn.Sequential(
        #転置畳み込み
        nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride),
        nn.BatchNorm2d(out_channels),
        nn.ReLU(inplace=True)
    )

"""## 損失関数について

セマンティックセグメンテーションの損失関数としてはBCELoss(Binary Cross Entropy)をベースとしたDiceBCELossがよく用いられます。詳細な説明とコードは以下に記載があります。https://www.kaggle.com/bigironsphere/loss-function-library-keras-pytorch

考え方としてはIoUに近く、予測した範囲が過不足なく教師データとなる領域を捉えているほど損失が低くなります。
"""

class DiceBCELoss(nn.Module):
    def __init__(self, weight=None, size_average=True):
        super(DiceBCELoss, self).__init__()

    def forward(self, inputs, targets, smooth=1):
        
        #comment out if your model contains a sigmoid or equivalent activation layer
        inputs = F.sigmoid(inputs)       
        
        #flatten label and prediction tensors
        inputs = inputs.view(-1)
        targets = targets.view(-1)
        
        intersection = (inputs * targets).sum()                            
        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  
        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')
        Dice_BCE = BCE + dice_loss
        
        return Dice_BCE


class DiceLoss(nn.Module):
    def __init__(self, weight=None, size_average=True):
        super(DiceLoss, self).__init__()

    def forward(self, inputs, targets, smooth=1):
        
        #comment out if your model contains a sigmoid or equivalent activation layer
        #inputs = F.sigmoid(inputs)       
        
        #flatten label and prediction tensors
        inputs = inputs.view(-1)
        targets = targets.view(-1)
        
        intersection = (inputs * targets).sum()                            
        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  
        
        return 1 - dice

"""セマンティックセグメンテーションの精度評価指標となるIoUのクラスを定義します。"""

class IoU(nn.Module):
    def __init__(self, weight=None, size_average=True):
        super(IoU, self).__init__()

    def forward(self, inputs, targets, smooth=1):
        inputs = inputs.view(-1)
        targets = targets.view(-1)
        intersection = (inputs * targets).sum()
        total = (inputs + targets).sum()
        union = total - intersection 
        
        IoU = (intersection + smooth)/(union + smooth)
                
        return IoU

"""続いてモデルを保存、ロードするための関数を定義します。"""

def save_ckp(state, is_best, checkpoint_path, best_model_path):
    f_path = checkpoint_path
    torch.save(state, f_path)
    if is_best:
        best_fpath = best_model_path
        shutil.copyfile(f_path, best_fpath)

def load_ckp(checkpoint_fpath, model, optimizer):
    checkpoint = torch.load(checkpoint_fpath)
    model.load_state_dict(checkpoint['state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer'])
    valid_loss_min = checkpoint['valid_loss_min']
    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()

if not os.path.exists(os.path.join(workDir,"model")):
    os.makedirs(os.path.join(workDir,"model"))

"""続いてU-Netの学習を行います。

まずモデル、オプティマイザ、損失関数のインスタンス作成を行います。

1エポック学習ごとに評価データによる精度評価を行い、精度評価の結果が最高のモデルをbest_model_path配下に保存する形になっています。
"""



#<---------------各インスタンス作成---------------------->
# Unet(3,1)の意味
# インプット画像は３チャンネル（RGB画像）
# 出力画像は1チャンネル（グレー画像）
model = UNet(3,1).cuda()

optimizer = torch.optim.Adam(model.parameters(),lr = 1e-3)
criterion = DiceLoss()
accuracy_metric = IoU()
valid_loss_min = np.Inf

checkpoint_path = os.path.join(workDir,'model/chkpoint_')
best_model_path = os.path.join(workDir,'model/bestmodel.pt')

total_train_loss = []
total_train_score = []
total_valid_loss = []
total_valid_score = []

losses_value = 0
for epoch in range(num_epochs):
  #<---------------トレーニング---------------------->
    train_loss = []
    train_score = []
    valid_loss = []
    valid_score = []
    pbar = tqdm(train_loader, desc = 'description')
    for x_train, y_train in pbar:
      x_train = torch.autograd.Variable(x_train).cuda()
      y_train = torch.autograd.Variable(y_train).cuda()
      optimizer.zero_grad()
      output = model(x_train)
      ## 損失計算
      loss = criterion(output, y_train)
      losses_value = loss.item()
      ## 精度評価
      score = accuracy_metric(output,y_train)
      loss.backward()
      optimizer.step()
      train_loss.append(losses_value)
      train_score.append(score.item())
      pbar.set_description(f"Epoch: {epoch+1}, loss: {losses_value}, IoU: {score}")
    #<---------------評価---------------------->
    with torch.no_grad():
      for image,mask in val_loader:
        image = torch.autograd.Variable(image).cuda()
        mask = torch.autograd.Variable(mask).cuda()
        output = model(image)
        ## 損失計算
        loss = criterion(output, mask)
        losses_value = loss.item()
        ## 精度評価
        score = accuracy_metric(output,mask)
        valid_loss.append(losses_value)
        valid_score.append(score.item())

    total_train_loss.append(np.mean(train_loss))
    total_train_score.append(np.mean(train_score))
    total_valid_loss.append(np.mean(valid_loss))
    total_valid_score.append(np.mean(valid_score))
    print(f"Train Loss: {total_train_loss[-1]}, Train IOU: {total_train_score[-1]}")
    print(f"Valid Loss: {total_valid_loss[-1]}, Valid IOU: {total_valid_score[-1]}")

    checkpoint = {
        'epoch': epoch + 1,
        'valid_loss_min': total_valid_loss[-1],
        'state_dict': model.state_dict(),
        'optimizer': optimizer.state_dict(),
    }
    
    # checkpointの保存
    save_ckp(checkpoint, False, checkpoint_path, best_model_path)
    
    # 評価データにおいて最高精度のモデルのcheckpointの保存
    if total_valid_loss[-1] <= valid_loss_min:
        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,total_valid_loss[-1]))
        save_ckp(checkpoint, True, checkpoint_path, best_model_path)
        valid_loss_min = total_valid_loss[-1]

    print("")

"""## ④U-Netモデルの性能評価の確認

### 出力したcsvで可視化
"""

import pandas as pd
import glob

# !ls training/論文掲載Ｍｏｄｅｌ結果
csvPaths = glob.glob("training/*_epoch100.csv")
csvPaths

df_score_conifer = pd.read_csv(csvPaths[0])
df_score_bamboo = pd.read_csv(csvPaths[1])
df_score_conifer.head(5)

df_score_conifer[["train_Loss",	"valid__Loss"]].max().max()
df_score_conifer[["train_Loss",	"valid__Loss"]].min().min()

import seaborn as sns

def plotScore(csvPath:str, strSize:int):

    df = pd.read_csv(csvPath)
    plt.figure(1)
    plt.figure(figsize=(15,5))
    sns.set_style(style="darkgrid")
    plt.subplot(1, 2, 1)
    sns.lineplot(x=range(1,len(df)+1), y=df["train_Loss"], label="Train Loss")
    sns.lineplot(x=range(1,len(df)+1), y=df["valid__Loss"], label="Valid Loss")
    # plt.title("Loss")
    plt.legend(fontsize=strSize)

    lossYrangeMax = df_score_conifer[["train_Loss",	"valid__Loss"]].max().max()
    lossYrangeMin = df_score_conifer[["train_Loss",	"valid__Loss"]].min().min()
    lossYrangeMax = round(lossYrangeMax, 2)
    lossYrangeMin = round(lossYrangeMin, 2)
    print(lossYrangeMin, lossYrangeMax)
    plt.yticks(np.arange(lossYrangeMin, lossYrangeMax+0.1, step=0.2))
    plt.xlabel("epochs",fontsize=strSize)
    plt.ylabel("DiceLoss",fontsize=strSize,labelpad=-20)
    plt.tick_params(labelsize=strSize)


    plt.subplot(1, 2, 2)
    sns.lineplot(x=range(1,len(df)+1), y=df["train_scoreIoU"], label="Train Score")
    sns.lineplot(x=range(1,len(df)+1), y=df["valid__scoreIoU"], label="Valid Score")
    # plt.title("Score (IoU)")
    plt.yticks(np.arange(0, 101, step=100))
    plt.legend(fontsize=strSize)
    plt.xlabel("epochs",fontsize=strSize)
    plt.ylabel("IoU",fontsize=strSize,labelpad=-30)
    plt.tick_params(labelsize=strSize)

    figPath = workDir + f"Unet_score_{os.path.basename(csvPath)}.png"
    plt.savefig(figPath,facecolor="azure", bbox_inches='tight', pad_inches=0)
    print(figPath)
    plt.show()

plotScore(csvPaths[0],30)
plotScore(csvPaths[1],30)

"""学習と評価が終了しましたので、エポックごとの損失、精度の変化をグラフ化します。"""

import seaborn as sns

plt.figure(1)
plt.figure(figsize=(15,5))
sns.set_style(style="darkgrid")
plt.subplot(1, 2, 1)
sns.lineplot(x=range(1,num_epochs+1), y=total_train_loss, label="Train Loss")
sns.lineplot(x=range(1,num_epochs+1), y=total_valid_loss, label="Valid Loss")
plt.title("Loss")
plt.xlabel("epochs")
plt.ylabel("DiceLoss")

plt.subplot(1, 2, 2)
sns.lineplot(x=range(1,num_epochs+1), y=total_train_score, label="Train Score")
sns.lineplot(x=range(1,num_epochs+1), y=total_valid_score, label="Valid Score")
plt.title("Score (IoU)")
plt.xlabel("epochs",fontsize=18)
plt.ylabel("IoU",fontsize=18)
plt.tick_params(labelsize=18)

plt.savefig(workDir + f"Unet_score_{modelID}.png")
plt.show()

score = {
    # "epoch" : range(1,num_epochs+1),
    "train_Loss" : total_train_loss,
    "valid__Loss" : total_valid_loss,
    "train_scoreIoU" : total_train_score,
    "valid__scoreIoU" : total_valid_score,
    }

import pandas as pd

df_score = pd.DataFrame(score, index=range(1,num_epochs+1))
df_score.to_csv(workDir + f"scoreSheet_{modelID}.csv")
df_score

"""以上の通り、エポックが進むにつれて損失が減り、精度が向上していることがわかります。これは機械学習においてはモデルの学習が進み、より汎化性能（予測性能）が増して行っていることを意味しています。

次に、作成した学習したモデルを利用して、実際のモデルによるセマンティックセグメンテーションの結果を表示してみましょう。

まず作成したモデルを読み込みます。
"""

# !ls training/classConifer_data440_batch22_epoch100

best_model_path = os.path.join("./training/classConifer_data440_batch22_epoch100",'model/bestmodel.pt')

model, optimizer, start_epoch, valid_loss_min = load_ckp(best_model_path, model, optimizer)



"""続いて入力画像と教師データ、モデルによる出力を表示する関数を用意し、出力を行います。"""

outImagePath = os.path.join(workDir,modelID,"OutImages")
os.makedirs(outImagePath,exist_ok=True)

def visualize_predict(model, n_images, imgSave=False):
    figure, ax = plt.subplots(nrows=n_images, ncols=3, figsize=(15, 5*n_images))
    with torch.no_grad():
        for data,mask in val_loader:
            data = torch.autograd.Variable(data, volatile=True).cuda()
            mask = torch.autograd.Variable(mask, volatile=True).cuda()
            o = model(data)
            break
    for img_no in tqdm(range(0, n_images)):
        tm=o[img_no][0].data.cpu().numpy()
        img = data[img_no].data.cpu()
        msk = mask[img_no].data.cpu()
        img = format_image(img)
        msk = format_mask(msk)

        msk = cv2.medianBlur(np.array(msk),11)

        outImagePath_pred = os.path.join(workDir,modelID,"OutImages",f"{img_no}_pred.png")
        outImagePath_img = os.path.join(workDir,modelID,"OutImages",f"{img_no}_org.png")
        outImagePath_msk = os.path.join(workDir,modelID,"OutImages",f"{img_no}_msk.png")

        # print(tm.shape, img.shape, msk.shape)
        cv2.imwrite(outImagePath_pred,tm)
        cv2.imwrite(outImagePath_img, img)
        cv2.imwrite(outImagePath_msk,np.array(msk))


        ax[img_no, 0].imshow(img)
        ax[img_no, 1].imshow(msk, interpolation="nearest", cmap="gray")
        ax[img_no, 2].imshow(tm, interpolation="nearest", cmap="gray")
        ax[img_no, 0].set_title("Input Image")
        ax[img_no, 1].set_title("Labeled Mask Conifer")
        ax[img_no, 2].set_title("Predicted Mask Conifer")
        ax[img_no, 0].set_axis_off()
        ax[img_no, 1].set_axis_off()
        ax[img_no, 2].set_axis_off()
    plt.tight_layout()
    if imgSave:
        plt.savefig(workDir + f"predictedSet_{modelID}.png")
    plt.show()

visualize_predict(model, 4, imgSave=True)
# visualize_predict(model, train_dataset.__len__()//20-1, imgSave=True)

outputImagesPaths = glob.glob("./training/classConifer_data440_batch22_epoch100/OutImages/*_org.png")
# outputImagesPaths

outputImagesPaths = sorted(outputImagesPaths)

print('./training/classConifer_data440_batch22_epoch100/OutImages/10_org.png')

def visualize_images(outputImagesPaths, LabesName:str):
    n_images = len(outputImagesPaths)
    figure, ax = plt.subplots(nrows=n_images, ncols=3, figsize=(15, 5*n_images))
    print(n_images)

    for orgPath in tqdm(outputImagesPaths):
        img_no = int(re.findall(".*/(\d+)_org.png",orgPath)[0])
        
        predPath = orgPath.replace("org","pred")
        mskPath = orgPath.replace("org","msk")
        tm = cv2.imread(predPath)*255
        img = cv2.imread(orgPath)
        msk = cv2.imread(mskPath)

        msk = cv2.medianBlur(np.array(msk),11)

        ax[img_no, 0].imshow(img)
        ax[img_no, 1].imshow(msk, interpolation="nearest", cmap="gray")
        ax[img_no, 2].imshow(tm, interpolation="nearest", cmap="gray")
        ax[img_no, 0].set_title("Input Image")
        ax[img_no, 1].set_title(f"Labeled Mask {LabesName}")
        ax[img_no, 2].set_title(f"Predicted Mask {LabesName}")
        ax[img_no, 0].axes.xaxis.set_visible(False)
        ax[img_no, 1].axes.xaxis.set_visible(False)
        ax[img_no, 2].axes.xaxis.set_visible(False)
        ax[img_no, 0].axes.yaxis.set_visible(False)
        ax[img_no, 1].axes.yaxis.set_visible(False)
        ax[img_no, 2].axes.yaxis.set_visible(False)

    plt.tight_layout()
    plt.savefig(os.path.join(os.path.dirname(orgPath), f"predictedSet_{LabesName}.png"))
    plt.show()

visualize_images(outputImagesPaths, "Conifer")

outputImagesPaths = glob.glob("./training/classBamboo_data880_batch44_epoch100/classBamboo_data880_batch44_epoch100/OutImages/*_org.png")
# outputImagesPaths

outputImagesPaths_bamoo = sorted(outputImagesPaths)

print(outputImagesPaths[0])


visualize_images(outputImagesPaths_bamoo, "Bamboo")

"""以上の結果を確認してみます。

Label Mask(教師データ)とPredicted Mask(モデル予測データ)を比較すると完全に等しくはなりませんが、モデルは比較的教師データに近いセグメンテーションをしていることがわかります。

U-Netはピクセルごとの画像の濃淡を元にセグメンテーションしているので、教師データより細かいセグメンテーションを行なっていることも分かるかと思います。

また、今回用いた学習データは数百枚のトレーニングデータしか用いておらず、（タスクにもよりますが）CNNなどで必要となるトレーニングデータの量と比べるととても少ないと感じられると思いますが、実際にデータ数が少ない時にも有用な方法です。（自分の研究ではそうでした。）

作成したモデルは保存していますので、このモデル再使用することでお手持ちの別の画像にもセグメンテーションをすることができますので、是非色々試して考察を行なって頂ければ幸いです。

ガチ勢の方はこんなものと思われるかもしれませんが、私も学生で勉強中の身という事で、ご勘弁くださいませ。。
"""

